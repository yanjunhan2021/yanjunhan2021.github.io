<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="../../jemdoc.css" type="text/css" />
<title>DS-GA 3001 (MATH-GA 2830): Information Theory for Statistics and Learning</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Information Theory</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="notes.html">Lecture notes</a></div>
<div class="menu-item"><a href="https://docs.google.com/document/d/1dNxs5iXuvH9AcdxSCjrsAp12ZdtR83Q-Y5sBKODp5Fs/edit?usp=sharing">Syllabus</a></div>
<div class="menu-item"><a href="homework.html">Homework</a></div>
<div class="menu-item"><a href="project.html">Project</a></div>
<div class="menu-category">External Links</div>
<div class="menu-item"><a href="https://piazza.com/nyu/fall2025/dsga3001">Piazza</a></div>
<div class="menu-item"><a href="https://www.gradescope.com/courses/1110155">Gradescope</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>DS-GA 3001 (MATH-GA 2830): Information Theory for Statistics and Learning</h1>
<div id="subtitle"><a href="https://www.nyu.edu">New York University</a>, 
<a href="https://yanjunhan2021.github.io">Yanjun Han</a>, 
Fall 2025</div>
</div>
<h2>Description</h2>
<ul>
<li><p>The interplay between information theory, statistics, and machine learning is a constant theme in the development of all fields. This course will discuss how techniques rooted in information theory play a key role in understanding the fundamental limits of statistical and machine learning problems in terms of minimax risk and sample complexity, and develop procedures that attain the statistical optimality. The primary focus is on information-theoretic applications to statistics and machine learning, rather than the classical “IEEE-style information theory”. </p>

<p> This course will cover the following topics: 1) entropy, mutual information, KL divergence, f-divergences, and their many applications (source coding, channel coding, adaptive data analysis, PAC Bayes, binary hypothesis testing, large deviation, strong converse, I-MMSE formula, area theorem, functional inequalities, etc); 2) techniques for minimax lower bounds (mutual information method; Le Cam, Assouad, and Fano; methods of second moment and orthogonal polynomials; metric entropy and global Fano; etc); 3) constructive procedures (entropic upper bounds of statistical estimation; redundancy, aggregation, prediction via universal compression; sampling via strong data-processing inequality; etc). Detailed examples under many areas will be provided for each topic. 
 </p>
</li>
</ul>
<h2>Time and location</h2>
<ul>
<li><p>Lecture: Mon 4:55 - 8:00 PM at GCASL (238 Thompson St), Room 261 </p></li>
<li><p>Office hour: Fri 4:00 - 5:00 PM at CDS 705, or by appointment </p></li>
</ul>
</td>
</tr>
</table>
</body>
</html>
